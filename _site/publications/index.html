<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Accountable Systems Lab - Publications</title>
  <meta name="description" content="Accountable Systems Lab -- Publications.">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/publications/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Accountable Systems Lab @ Carnegie Mellon University</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/team">Team</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h1 id="publications">Publications</h1>

<h2 id="spotlight-papers">Spotlight Papers</h2>
<!--
(For a full list see [below](#full-list) or go to [Google Scholar](https://scholar.google.ch/citations?user=TqxYWZsAAAAJ), [ResearcherID](https://www.researcherid.com/rid/D-7763-2012)) -->

<!--  -->

<!--  -->

<!--  -->
<div class="row">
  <!--  -->

  <div class="col-sm-12 clearfix">
    <div class="well">
      <pubtit>Use Privacy in Data-Driven Systems: Theory and Experiments with Machine Learnt Programs</pubtit>
      <p><img src="http://localhost:4000/images/pubpic/project_proxyuse.png" class="img-responsive" width="33%" style="float: left" /></p>
      <p>This paper presents an approach to formalizing and enforcing a class of use privacy properties in data-driven systems. In contrast to prior work, we focus on use restrictions on proxies (i.e. strong predictors) of protected information types. Our definition relates proxy use to intermediate computations that occur in a program, and identify two essential properties that characterize this behavior: 1) its result is strongly associated with the protected information type in question, and 2) it is likely to causally affect the final output of the program. For a specific instantiation of this definition, we present a program analysis technique that detects instances of proxy use in a model, and provides a witness that identifies which parts of the corresponding program exhibit the behavior. Recognizing that not all instances of proxy use of a protected information type are inappropriate, we make use of a normative judgment oracle that makes this inappropriateness determination for a given witness. Our repair algorithm uses the witness of an inappropriate proxy use to transform the model into one that provably does not exhibit proxy use, while avoiding changes that unduly affect classification accuracy. Using a corpus of social datasets, our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques, and subsequently remove them while maintaining acceptable classification performance</p>
      <p><em>
  Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, Shayak Sen,
  </em></p>
      <p><strong><a href="https://arxiv.org/abs/1705.07807">paper</a></strong></p>
      <p class="text-danger"><strong> </strong></p>
      <p> </p>
    </div>
  </div>

  <!--  -->

  <!--  -->

  <!--  -->

  <!--  -->

  <div class="col-sm-12 clearfix">
    <div class="well">
      <pubtit>Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination</pubtit>
      <p><img src="http://localhost:4000/images/pubpic/project_adfisher.png" class="img-responsive" width="33%" style="float: left" /></p>
      <p>To partly address people’s concerns over web tracking, Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users. We present AdFisher, an automated tool that explores how user behaviors, Google’s ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. Our methodology uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results. We use AdFisher to find that the Ad Settings was opaque about some features of a user’s profile, that it does provide some choice on ads, and that these choices can lead to seemingly discriminatory ads. In particular, we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page. We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male. We cannot determine who caused these findings due to our limited visibility into the ad ecosystem, which includes Google, advertisers, websites, and users. Nevertheless, these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies.</p>
      <p><em>
  Amit Datta, Michael Carl Tschantz, Anupam Datta,
  </em></p>
      <p><strong><a href="http://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml">paper</a></strong></p>
      <p class="text-danger"><strong> </strong></p>
      <p> </p>
    </div>
  </div>

  <!--  -->

  <!--  -->
</div>
<!--  -->

<p>   </p>

<h2 id="full-list">Full List</h2>

<p>Use Privacy in Data-Driven Systems: Theory and Experiments with Machine Learnt Programs <br />
  <em>Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, Shayak Sen </em><br /><a href="https://arxiv.org/abs/1705.07807">paper</a></p>

<p>Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination <br />
  <em>Amit Datta, Michael Carl Tschantz, Anupam Datta </em><br /><a href="http://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml">paper</a></p>


</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-4">

		  <p>&copy 2018 Accountable Systems Lab. Site made with <a href="https://jekyllrb.com">Jekyll</a>;  <a href="http://www.allanlab.org/">Site Adapt from Allan Lab.</a></p>
              <p>We are part of the <a href="https://www.ece.cmu.edu/">ECE Department</a> and <a href = "https://www.cs.cmu.edu/">School of Computer Science</a> at <a href="https://www.cmu.edu">Carnegie Mellon University</a>.</p>


		   <p>  </p><p>


		</div>
    <div class="col-sm-4">
		  Silicon Valley Contact:<br />

		</div>
		<div class="col-sm-4">
		  Pittsburgh Contact:<br />
		  [Todo]
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
